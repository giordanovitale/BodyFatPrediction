# ++++++++++++ REGRESSION ANALYSIS ++++++++++++

Apply the train-test split.
```{r}
sample <- sample(c(TRUE, FALSE), 
                 nrow(bodyfat), 
                 replace=TRUE, 
                 prob=c(0.75,0.25))
train  <- bodyfat[sample, ]
test   <- bodyfat[!sample, ]
```


### A) Linear Regression ###

- LINEAR REGRESSION USING PRINCIPAL COMPONENTS [?]

- LINEAR REGRESSION USING ALL THE VARIABLES
```{r}
# Train the model
complete_lm <- lm(BodyFat ~ ., data = train)

# summary of the model
summary(complete_lm)

# see Salini's additional code in file 1
```
Compute measure of goodness.
```{r}
# R Squared
summary(complete_lm)$r.squared

# Adjusted R Squared
summary(complete_lm)$adj.r.squared

# BIC
BIC(complete_lm)

#AIC 
AIC(complete_lm)
```

- ROBUST REGRESSION USING ALL THE VARIABLES
```{r}
# Train the model
robust_lm <- rlm(BodyFat ~ ., data = train, psi = psi.hampel, init = "lts")

# Summary
summary(robust_lm)

# BIC
BIC(robust_lm)
```



### B) LOGISTIC REGRESSION MODEL ###
```{r}
logistic_regression <- glm(BodyFat ~ ., data = train)
summary(logistic_regression)


# McFadden R squared
McF <- round(with(summary(logistic_regression), 1 - deviance/null.deviance),
             digits = 2)
paste("McFadden R squared for logistic regression is ", McF)
```


### C) - SUBSET SELECTION ###
(http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/155-best-subsets-regression-essentials-in-r/)

```{r}
# train the models
subset_selection <- regsubsets(BodyFat ~ ., data = train, method = "exhaustive", nvmax = 11)

# summary
subset_selection_summary <- summary(subset_selection)
subset_selection_summary
```

```{r}
# optimal number of features to include in the model, according to Cp
which.min(subset_selection_summary$cp)
```

```{r}
# visualize the performance of each "best performing" model with n features (n = 1, ..., 11)
plot(subset_selection_summary$cp, 
     xlab = "Number of Variables",
     ylab = "Adj R Squared",
     main = "Adj-R-Sqared for each best performing model")

points(9, 
       subset_selection_summary$cp[9],
       pch = 0,
       col = "red",
       cex = 2)
```

We saw that the best number of variables to include was 9 according to the Cp measurement.
What about AdjustedRSquared and BIC?
```{r}
data.frame(
  Adj.R2 = which.max(subset_selection_summary$adjr2),
  CP = which.min(subset_selection_summary$cp),
  BIC = which.min(subset_selection_summary$bic)
)
```
Show the coefficients of the best model according to Cp and Adj-R.Squared.
```{r}
coef(subset_selection, 9)

subset_selection_summary$which[9, ]
```

Show the coefficients of the best model according to BIC.
```{r}
coef(subset_selection, 6)

subset_selection_summary$which[6, ]
```

__Comment: even if 9 is the best number of features according to 2 goodness measurements, the improvement from 6 to 9 is negligible and for this reason, for higher simplicity, the model with 6 features may be preferred.__

Formally define the model with 9 variables (found through Cp and Adj-R-Sq)
```{r}
subset9 <- lm(BodyFat ~ Age + Weight + Height + 
                       Neck + Knee + Forearm +
                        Wrist + ACratio + HTratio,
              data = train) 

summary(subset9)
```

Formally define the model with 6 variables (found through BIC)
```{r}
subset6 <- lm(BodyFat ~ Age + Weight + Height + 
                        Forearm + Wrist + ACratio,
              data = train) 

summary(subset6)
```



### D) POLYNOMIAL REGRESSION  ###

Visualize BodyFat as a function of the two variables created, since the scatter plots have been performed before the feature engineering part.
```{r}
plot(bodyfat$BodyFat, bodyfat$HTratio)

plot(bodyfat$BodyFat, bodyfat$ACratio)
```

Create a polynomial model where the variables are the normal ones and the squared ones.
```{r}
polynomial_model <- lm(BodyFat ~ 
                         Age + I(Age^2) + Weight + I(Weight^2) + 
                         Height + I(Height^2) + Neck + I(Neck^2) + 
                         Knee + I(Knee^2) + Ankle + I(Ankle^2) + 
                         Biceps + I(Biceps^2) + Forearm + I(Forearm^2) + 
                         Wrist + I(Wrist^2) + ACratio + I(ACratio^2) +  
                         HTratio + I(HTratio^2), 
                       data=train)

summary(polynomial_model)
```

Perform exhaustive subset selection technique to the polynomial model.
```{r}
subset_polynomial <- regsubsets(BodyFat ~ 
                         Age + I(Age^2) + Weight + I(Weight^2) + 
                         Height + I(Height^2) + Neck + I(Neck^2) + 
                         Knee + I(Knee^2) + Ankle + I(Ankle^2) + 
                         Biceps + I(Biceps^2) + Forearm + I(Forearm^2) + 
                         Wrist + I(Wrist^2) + ACratio + I(ACratio^2) +  
                         HTratio + I(HTratio^2), 
                       data=train,
                       method = "exhaustive",
                       nvmax = 22)

subset_polynomial_summary <- summary(subset_polynomial)
```

Which is the optimal number of features according to the Cp measure?
```{r}
x_pol <- which.min(subset_polynomial_summary$cp)
x_pol
```

```{r}
# visualize the performance of each "best performing" model with n features (n = 1, ..., 11)
plot(subset_polynomial_summary$cp, 
     xlab = "Number of Variables",
     ylab = "Adj R Squared",
     main = "Adj-R-Sqared for each best performing model")

points(x_pol, 
       subset_polynomial_summary$cp[x_pol],
       pch = 0,
       col = "red",
       cex = 2)
```


```{r}
coef(subset_polynomial, 9)

subset_polynomial_summary$which[9, ]
```


```{r}
naive_polyn <- lm(BodyFat ~ Age + Weight + I(Weight^2) + Height + I(Height^2)
                  + Wrist + I(Wrist^2) + ACratio + I(ACratio^2),
                  data = train)

summary(naive_polyn)
```


```{r}
BIC(naive_polyn)
```

Bad resutls!
```{r}
vif(naive_polyn)
```



### E) PARTIAL LEAST SQUARES ###
```{r}
pls_model <- plsr(BodyFat ~., data = train, scale = TRUE, validation = "CV")
summary(pls_model)
```
VALIDATION: RMSEP (Root Mean Squared Error of Prediction): This section presents the cross-validated performance of the PLS model using the RMSEP metric. The RMSEP indicates the average prediction error of the model on new data. The values under each number of components indicate the RMSEP obtained after cross-validation for that specific number of components.

TRAINING: % Variance Explained: This section shows the percentage of variance explained by the PLS model for both the predictor variables (X) and the response variable "BodyFat" (Y). For example, with 1 component, the PLS model explains 46.61% of the variation in the predictor variables and 49.11% of the variation in "BodyFat."

```{r}
validationplot(pls_model, val.type="MSEP", legendpos = "topright", main="PLSR Test")
```

Build a model made by the two components.
```{r}
# Extract the scores (component values) for the first two components
component_scores <- pls_model$scores[, 1:2]

# Create a new data frame with the component scores and the response variable 'BodyFat'
data_with_components <- data.frame(component1 = component_scores[, 1],
                                   component2 = component_scores[, 2],
                                   BodyFat = train$BodyFat)

# Fit a linear regression model using the first two components as predictors
model_pls_components <- lm(BodyFat ~ component1 + component2, data = data_with_components)
summary(model_pls_components)
```

```{r}
BIC(model_pls_components)
```




### F) STEP FUNCTIONS: NO ###



### G) LOCAL REGRESSION ###
```{r}
#define k-fold cross validation method
ctrl <- trainControl(method = "cv", number = 5)
grid <- expand.grid(span = seq(0.5, 0.9, len = 5), degree = 1)

#perform cross-validation using smoothing spans ranging from 0.5 to 0.9
model <- train(BodyFat ~ ., data = train, method = "gamLoess", tuneGrid=grid, trControl = ctrl)

#print results of k-fold cross-validation
print(model)
```




### H) REGRESSION TREES ###

https://www.statology.org/classification-and-regression-trees-in-r/

```{r}
library(rpart)
library(rpart.plot)

regression_tree <- rpart(BodyFat ~ ., data = train, cp = 0.001)

printcp(regression_tree)
```

```{r}
rpart.plot::rpart.plot(regression_tree,
                       main = "Plot of the complete regression tree")
```

Prune the tree
```{r}
# find the optimal Cp value
best_cp <- regression_tree$cptable[which.min(regression_tree$cptable[, "xerror"]), "CP"]

# pruned tree
pruned_tree <- prune(regression_tree, cp = best_cp)

# plot the pruned tree
rpart.plot::rpart.plot(pruned_tree,
                       faclen = 0,
                       type = 2,
                       box.palette = "Greens",
                       main = "Plot of the Pruned Tree")
```
What to do with this? 
1) prediction: later
2) interpretation: e.g., individuals with high HT ratio and AC ratio suffer high bodyfat...)
3) feature importance: the features appearing in the plot are more important
4) terminal node scatterplots, or decision boundaries, not sure doable with continuous Y variable...



### I) GAMS ###

library gam or library mgcv
```{r}
gam_model <- gam(BodyFat ~ s(Age) + (Weight) + (Height) + 
                            s(Neck) + s(Knee) + s(Ankle) + 
                            s(Biceps) + s(Forearm) + (Wrist) +
                            (ACratio) + s(HTratio),
                 data = train)

summary(gam_model)
```


```{r}
plot(gam_model)
```

```{r}
BIC(gam_model)
```


Using the other library
```{r}
library(mgcv)

gam_model2 <- gam(BodyFat ~ s(Age) + (Weight) + (Height) + 
                            s(Neck) + s(Knee) + s(Ankle) + 
                            s(Biceps) + s(Forearm) + (Wrist) +
                            (ACratio) + s(HTratio),
                 data = train)

summary(gam_model2)
```

```{r}
BIC(gam_model2)
```

